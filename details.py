import os
import json
from enum import StrEnum
from typing import Optional

from openai import AzureOpenAI
from pydantic import BaseModel, Field


class Steps(StrEnum):
    _1="ASK_CURRENT_EMPLOYMENT"
    _2="ASK_WORK_LOCATION"
    _3="ASK_HOME_LOCATION"
    _4="ASK_RELOCATION"
    _5="HANDLE_RELOCATION_PREFERENCE"
    _6="ASK_LAST_SALARY"
    _7="ASK_EXPECTED_SALARY"
    _8="REQUEST_RESUME"
    _9="CONVERSATION_COMPLETE"


class ApplicantDetails(BaseModel):
    is_currently_employed: Optional[bool] = Field(
        default=None, description="Whether the user is currently employed or not."
    )
    work_location: Optional[str] = Field(description="The user's work location")
    home_location: Optional[str] = Field(description="The user's home location")
    willing_to_relocate: Optional[bool] = Field(default=None, description="Whether the user is open to relocation")
    last_drawn_salary: Optional[int] = Field(default=None, description="The user's last drawn salary")
    expected_salary: Optional[int] = Field(default=None, description="The user's expected salary")


class LLMResponse(BaseModel):
    response_to_user: str = Field(
        ...,
        description="The response generated by the LLM",
    )
    updated_data: ApplicantDetails = Field(
        ..., 
        description="The updated applicant details after LLM processing"
    )
    next_step: Steps = Field(
        default=Steps._1, description="The next step to be taken after LLM processing"
    )
    is_complete: bool = Field(
        default=False, description="Whether the LLM processing is complete"
    )


client = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT", ""),
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
)
system_prompt = """
You are a friendly and efficient recruitment assistant chatbot. Your primary goal is to guide a user through a job application pre-screening process. You must follow a specific conversational flow, asking only ONE question at a time.

You will be given the conversation history and a JSON object called `current_data` which holds the information collected so far. Your task is to analyze the user's latest response, update the `current_data` object, and determine the next question to ask based on the rules below.

**Conversation Flow Logic:**
1.  **`ASK_CURRENT_EMPLOYMENT`**: Start by asking "Sure, are you working anywhere right now?".
2.  **`ASK_WORK_LOCATION`**: If the user is currently employed (`is_currently_employed` is true), ask "Which (city/district) are you working in?".
3.  **`ASK_HOME_LOCATION`**: If the user is NOT currently employed (`is_currently_employed` is false), ask "Where do you currently live?".
4.  **`ASK_RELOCATION`**: After getting either the work or home location, ask "Are you open to moving to a different city for a job?".
5.  **`HANDLE_RELOCATION_PREFERENCE`**: Acknowledge their relocation preference.
    - If `willing_to_relocate` is true, say something like "Sure, I will show you jobs in other cities as well."
    - If `willing_to_relocate` is false, say something like "Sure, I will show you jobs only in your city."
    - Immediately after this acknowledgement, proceed to the next question in the same turn.
6.  **`ASK_LAST_SALARY`**: After handling relocation, ask "What's your last drawn salary?".
7.  **`ASK_EXPECTED_SALARY`**: After getting the last salary, ask "What's your expected salary?".
8.  **`REQUEST_RESUME`**: After getting the expected salary, ask "Alright, please send your resume.".
9.  **`CONVERSATION_COMPLETE`**: Once the resume is requested, the flow is complete.

**Your Instructions:**
- Adhere strictly to the flow. Do not skip questions or ask them out of order.
- Be conversational and friendly, but concise.
- Your entire response MUST be a single, valid JSON object. Do not add any text before or after the JSON.
- If the user provides information for a future step (e.g., they mention their expected salary early), extract it and store it, but still ask the question when you get to that step in the flow to confirm.
"""

llm_schema = {
    "type": "json_schema",
    "json_schema": {
        "name": "ApplicantSchema",
        "schema": LLMResponse.model_json_schema(),
    },
}


#### START###
user_info = None
messages = [        
    {
        "role": "system",
        "content": system_prompt,
    }
]
user_message = "Hi"
messages.append(
    {
        "role": "user",
        "content": f"user message: {user_message}, current_data: {user_info}",
    }
)
x = client.chat.completions.create(
    model="gpt-4.1",
    temperature=0,
    messages=messages,
    response_format=llm_schema,  # type: ignore
)
response = LLMResponse(**json.loads(x.choices[0].message.content))
messages.append(
    {
        "role": "assistant",
        "content": f"{response.response_to_user}",
    }
)
user_info = response.updated_data.model_dump()
print(response)
print(messages)

## 1
user_message = "Yes"
messages.append(
    {
        "role": "user",
        "content": f"user message: {user_message}, current_data: {user_info}",
    }
)
x = client.chat.completions.create(
    model="gpt-4.1",
    temperature=0,
    messages=messages,
    response_format=llm_schema,  # type: ignore
)
response = LLMResponse(**json.loads(x.choices[0].message.content))
messages.append(
    {
        "role": "assistant",
        "content": f"{response.response_to_user}",
    }
)
user_info = response.updated_data.model_dump()
print(response)
print(messages)

## 2
user_message = "Bangalore"
messages.append(
    {
        "role": "user",
        "content": f"user message: {user_message}, current_data: {user_info}",
    }
)
x = client.chat.completions.create(
    model="gpt-4.1",
    temperature=0,
    messages=messages,
    response_format=llm_schema,  # type: ignore
)
response = LLMResponse(**json.loads(x.choices[0].message.content))
messages.append(
    {
        "role": "assistant",
        "content": f"{response.response_to_user}",
    }
)
user_info = response.updated_data.model_dump()
print(response)
print(messages)

## 3
user_message = "No"
messages.append(
    {
        "role": "user",
        "content": f"user message: {user_message}, current_data: {user_info}",
    }
)
x = client.chat.completions.create(
    model="gpt-4.1",
    temperature=0,
    messages=messages,
    response_format=llm_schema,  # type: ignore
)
response = LLMResponse(**json.loads(x.choices[0].message.content))
messages.append(
    {
        "role": "assistant",
        "content": f"{response.response_to_user}",
    }
)
user_info = response.updated_data.model_dump()
print(response)
print(messages)

## 4
user_message = "ok"
messages.append(
    {
        "role": "user",
        "content": f"user message: {user_message}, current_data: {user_info}",
    }
)
x = client.chat.completions.create(
    model="gpt-4.1",
    temperature=0,
    messages=messages,
    response_format=llm_schema,  # type: ignore
)
response = LLMResponse(**json.loads(x.choices[0].message.content))
messages.append(
    {
        "role": "assistant",
        "content": f"{response.response_to_user}",
    }
)
user_info = response.updated_data.model_dump()
print(response)
print(messages)

## 5
user_message = "17000"
messages.append(
    {
        "role": "user",
        "content": f"user message: {user_message}, current_data: {user_info}",
    }
)
x = client.chat.completions.create(
    model="gpt-4.1",
    temperature=0,
    messages=messages,
    response_format=llm_schema,  # type: ignore
)
response = LLMResponse(**json.loads(x.choices[0].message.content))
messages.append(
    {
        "role": "assistant",
        "content": f"{response.response_to_user}",
    }
)
user_info = response.updated_data.model_dump()
print(response)
print(messages)

# 6
user_message = "20000"
messages.append(
    {
        "role": "user",
        "content": f"user message: {user_message}, current_data: {user_info}",
    }
)
x = client.chat.completions.create(
    model="gpt-4.1",
    temperature=0,
    messages=messages,
    response_format=llm_schema,  # type: ignore
)
response = LLMResponse(**json.loads(x.choices[0].message.content))
messages.append(
    {
        "role": "assistant",
        "content": f"{response.response_to_user}",
    }
)
user_info = response.updated_data.model_dump()
print(response)
print(messages)

# 7
user_message = "Here is my resume"
messages.append(
    {
        "role": "user",
        "content": f"user message: {user_message}, current_data: {user_info}",
    }
)
x = client.chat.completions.create(
    model="gpt-4.1",
    temperature=0,
    messages=messages,
    response_format=llm_schema,  # type: ignore
)
response = LLMResponse(**json.loads(x.choices[0].message.content))
messages.append(
    {
        "role": "assistant",
        "content": f"{response.response_to_user}",
    }
)
user_info = response.updated_data.model_dump()
print(response)
print(messages)
